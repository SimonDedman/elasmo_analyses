# Investigation Summary: Priorities 1, 2, & 4

**Date:** 2025-10-23
**Priorities Investigated:**
1. Fix Sci-Hub Downloads
2. Semantic Scholar DOI Discovery
3. IUCN Web Scraping

**Status:** Investigations complete - Ready for next actions

---

## Executive Summary

All three high-priority acquisition strategies have been investigated. Here are the key findings:

| Priority | Status | Potential Gain | Difficulty | Recommended Action |
|----------|--------|----------------|------------|-------------------|
| **Priority 1: Sci-Hub** | üî¥ Blocked | +8,000 PDFs | Medium-High | Implement Tor-based solution or wait |
| **Priority 2: Semantic Scholar** | ‚ö†Ô∏è Low Yield | +19 DOIs | Low | Extract and try alternative sources |
| **Priority 4: IUCN** | ‚ö†Ô∏è API Failed | +600-800 PDFs | Medium | Build web scraper instead of API |

---

## Priority 1: Sci-Hub - BLOCKED &#x1F534;

### Current Situation
- **Attempts:** 11,858 papers with DOIs
- **Successes:** 1,609 (13.6%)
- **Expected:** 8,000-10,000 (70-85%)
- **Gap:** ~8,000 PDFs missing

### Root Causes Identified

#### 1. IP Blocking (32.4% of failures - 3,842 papers)
- All Sci-Hub mirrors returning "ERROR" or HTTP 403
- Even previously successful DOIs now fail
- Systematic blocking after rapid bulk downloads

**Evidence:**
```
https://sci-hub.se/10.1242/jeb.059667 ‚Üí "ERROR" (5 bytes response)
https://sci-hub.ru/10.1242/jeb.059667 ‚Üí "ERROR" (5 bytes response)
https://sci-hub.wf/10.1242/jeb.059667 ‚Üí Empty response (0 bytes)
```

#### 2. PDF Extraction Failure (52.1% of failures - 6,183 papers)
- Script gets HTTP 200 but cannot find PDF link
- Looking for `<embed type="application/pdf">` tags
- Page structure may have changed or requires JavaScript

### Mirrors Tested

| Mirror | Homepage Status | DOI Access | Notes |
|--------|----------------|------------|-------|
| sci-hub.st | ‚úÖ Online | ‚ùå "ERROR" | Blocking automated requests |
| sci-hub.se | ‚úÖ Online | ‚ùå "ERROR" | Blocking automated requests |
| sci-hub.ru | ‚úÖ Online | ‚ùå "ERROR" | Blocking automated requests |
| sci-hub.wf | ‚úÖ Online | ‚ùå Empty | Different blocking method |
| sci-hub.cat | ‚úÖ Online | ‚ùì Not tested | - |
| sci-hub.ren | ‚úÖ Online | ‚ùì Not tested | - |
| sci-hub.ee | ‚ùå Timeout | ‚ùå N/A | Not accessible |

### Year-Based Success Patterns

| Year Range | Attempts | Successes | Success Rate |
|------------|----------|-----------|--------------|
| 2025 | ~1,000 | ~100 | ~10% |
| 2024 | ~1,500 | ~200 | ~13% |
| 2023 | ~1,000 | ~150 | ~15% |
| 2013-2021 | ~4,500 | 0 | 0% |
| 2012 | 485 | 38 | 7.8% |

**Pattern:** Success only with very recent papers (2023-2025), suggesting download occurred before full blocking kicked in.

### Recommended Solutions (In Order of Preference)

#### Option A: Tor Network Integration (RECOMMENDED)
**Effort:** Medium (1-2 days)
**Cost:** Free
**Success Probability:** High (70-80%)

**Approach:**
```python
# Install: sudo apt-get install tor python3-stem
import socks
import socket

# Route through Tor
socks.set_default_proxy(socks.SOCKS5, "localhost", 9050)
socket.socket = socks.socksocket

# Requests will now use Tor
# Can rotate identity periodically
```

**Pros:**
- Bypasses IP blocks completely
- Free and open-source
- Can rotate IPs automatically
- Ethical solution

**Cons:**
- Slower downloads (~2-3x slower)
- Requires Tor installation and configuration
- May still face CAPTCHA challenges

**Implementation Steps:**
1. Install Tor (`sudo apt install tor`)
2. Install stem library (`pip install stem`)
3. Modify `download_via_scihub.py` to route through Tor
4. Test with 10 sample DOIs
5. If successful, re-run all 11,858 DOIs
6. Rotate Tor identity every 100-200 requests

**Timeline:** 2-3 days including testing

---

#### Option B: Wait for IP Block Expiration
**Effort:** None
**Cost:** Free
**Success Probability:** Medium (30-50%)

**Approach:**
- Wait 7-14 days for potential IP block expiration
- Test from different network (mobile hotspot, VPN, friend's network)
- If working from alternate network, continue downloads from there

**Pros:**
- No development effort
- May work if block is temporary

**Cons:**
- Uncertain timeline
- Block may be permanent
- Delays other work

---

#### Option C: Browser Automation (Selenium/Playwright)
**Effort:** High (3-5 days)
**Cost:** Free
**Success Probability:** High (80-90%) but slow

**Approach:**
```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(headless=False)
    page = browser.new_page()
    page.goto(f"https://sci-hub.se/{doi}")
    # Handle CAPTCHAs manually or with 2captcha.com
    # Extract PDF from page
```

**Pros:**
- Looks like real browser traffic
- Can handle CAPTCHAs (with manual intervention)
- Highest success rate

**Cons:**
- Very slow (~30-60 seconds per paper)
- Would take ~100-200 hours for 11,858 papers
- Requires manual CAPTCHA solving or paid service
- High complexity

---

#### Option D: Alternative Sources (LibGen, Anna's Archive)
**Effort:** Low-Medium (1-2 days)
**Cost:** Free
**Success Probability:** Low-Medium (20-40%)

**Approach:**
- Try Library Genesis (LibGen) for DOI-based lookup
- Try Anna's Archive
- Try Z-Library (if accessible)

**Pros:**
- May have papers Sci-Hub doesn't
- Less likely to be blocked
- Multiple fallback options

**Cons:**
- Lower coverage than Sci-Hub
- Each service has different API/structure
- Still subject to blocking

---

### Immediate Next Steps for Priority 1

1. **Manual Test (15 minutes)**
   - Open browser, go to https://sci-hub.se manually
   - Test a known DOI: `10.1242/jeb.059667`
   - Check if CAPTCHA appears
   - Document actual page structure

2. **Try from Different Network (30 minutes)**
   ```bash
   # Use mobile hotspot or VPN
   python3 scripts/download_via_scihub.py --max-papers 10
   ```

3. **If still blocked, implement Tor solution (1-2 days)**

4. **Test Tor solution with 100 papers before full run**

---

## Priority 2: Semantic Scholar - LOW YIELD ‚ö†Ô∏è

### Results

- **Papers searched:** 13,890 (papers without DOIs)
- **Papers found:** 24 (0.17%)
- **DOIs discovered:** 19 (0.14%)
- **PDFs downloaded:** 6 (0.04%)

### Status Breakdown

| Status | Count | Percentage |
|--------|-------|------------|
| not_found | 13,866 | 99.8% |
| no_pdf_available | 13 | 0.1% |
| success | 6 | 0.04% |
| error | 5 | 0.04% |

### New DOIs Discovered (19 total)

These DOIs were NOT in the Sci-Hub log, meaning they're genuinely new discoveries:

```
10.1038/s41598-018-38270-3 - Using DNA Barcoding...
10.4067/s0717-95022025000100123 - Miolog√≠a de la Regi√≥n Cef√°lica...
10.24275/UAM/IZT/DCBS/HIDRO/2019V29N1/SANCHEZ - Relation between the sharpnose shark...
10.2110/carnets.2021.2117 - The ichnospecies Linichnus bromleyi...
10.37828/em.2019.24.2 - First photographic inland record...
10.1002/ar.25693 - Functional models from limited data...
10.51400/2709-6998.1594 - Effectiveness Against White Sharks...
10.37570/bgsd-2018-66-03 - New fossil fish microremains...
10.1177/0971102320190201 - Micro and Mega-Vertebrate Fossils...
10.18475/cjos.v54i1.a5 - Spinner Dolphin with Evidence...
... and 9 more
```

### Analysis

**Why So Low?**
1. **Papers without DOIs** are predominantly grey literature:
   - Conference abstracts
   - Thesis chapters
   - Society reports
   - Local journals
   - Books/book chapters
2. **Not indexed in Semantic Scholar** - Academic search engines focus on peer-reviewed literature
3. **Title matching difficult** - Many grey literature titles are generic or in foreign languages

**Papers with Good Matches (>0.8 similarity): 23**
- 18 of these have DOIs
- 0 had PDFs available for download
- Matches found but content paywalled or not digitized

### Value Assessment

**Direct PDF Acquisition:** ‚ùå Very low (6 PDFs = 0.04%)
**DOI Discovery:** ‚ö†Ô∏è Minimal (19 DOIs = 0.14%)
**Worth Continuing:** ‚ùå No - diminishing returns

### Recommended Actions for Priority 2

1. **Extract the 19 new DOIs** and try them via:
   - Direct publisher access (institutional login)
   - Google Scholar search
   - ResearchGate/Academia.edu
   - Manual download

2. **Abandon further Semantic Scholar searching** for this dataset
   - 99.8% "not found" rate
   - Better to focus effort elsewhere

3. **Save DOIs for future use:**
   ```bash
   # Extract DOIs from semantic_scholar_log.csv
   python3 << 'EOF'
   import pandas as pd
   log = pd.read_csv('logs/semantic_scholar_log.csv')
   new_dois = log[log['doi'].notna()][['literature_id', 'doi', 'matched_title']]
   new_dois.to_csv('outputs/semantic_scholar_discovered_dois.csv', index=False)
   print(f"Saved {len(new_dois)} DOIs to outputs/semantic_scholar_discovered_dois.csv")
   EOF
   ```

---

## Priority 4: IUCN - API FAILED, WEB SCRAPING NEEDED ‚ö†Ô∏è

### Current Situation

- **Attempts:** 1,104 papers (identified as species assessments)
- **Success:** 0 (0%)
- **All returned:** "Species not found in IUCN database"

### Why API Approach Failed

**Script attempted:** `scripts/download_iucn_assessments.py`

**Approach:**
1. Extract species name from paper title using regex
2. Query IUCN API: `https://apiv3.iucnredlist.org/api/v3/species/{species_name}`
3. Download PDF if available

**Failures:**
1. **Species name extraction imperfect** - Titles like "Red List Assessment of Sharks in South Africa" don't contain specific species names
2. **IUCN API doesn't provide PDFs** - API returns JSON metadata, not PDF links
3. **Many papers are about Red List in general** - Not individual species assessments

### Sample Failed Lookups

```
Heterodontus omanensis ‚Üí not_found_in_iucn
Rhinobatos punctifer ‚Üí not_found_in_iucn
Amblyraja reversa ‚Üí not_found_in_iucn
Carcharhinus falciformis ‚Üí not_found_in_iucn (this species definitely exists in IUCN!)
```

**Issue:** The API may require exact formatting, or assessment PDFs aren't accessible via API.

### Actual IUCN Red List Structure

**Website:** https://www.iucnredlist.org/

**Species Assessment URLs follow pattern:**
```
https://www.iucnredlist.org/species/{taxon_id}/{species-name}
```

**Example:**
```
Carcharodon carcharias (Great White Shark)
https://www.iucnredlist.org/species/3855/2878674
```

**Assessment PDFs:**
- Available on species pages as "Download PDF" button
- Not directly accessible via API
- Requires web scraping or direct URL construction

### Papers We're Looking For

**Types of IUCN-related papers in our database (~1,104):**

1. **Individual species assessments** (~300-400)
   - "Carcharodon carcharias Red List Assessment"
   - These ARE available as PDFs on IUCN website

2. **Regional Red List reports** (~200-300)
   - "Red List of Elasmobranchs of the Mediterranean Sea"
   - Available as full reports on IUCN website

3. **General Red List methodology papers** (~400-500)
   - "Using Red List criteria for conservation planning"
   - May not be PDFs, may be published journal articles

### Recommended Web Scraping Approach

#### Strategy A: Scrape Individual Species Assessments

**Steps:**
1. Identify papers with specific species names in titles
2. Search IUCN website for species
3. Get taxon ID from search results
4. Download PDF from species page

**Example Implementation:**
```python
import requests
from bs4 import BeautifulSoup

def get_iucn_assessment_pdf(species_name):
    # Search for species
    search_url = f"https://www.iucnredlist.org/search?query={species_name}"
    response = requests.get(search_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find species page link
    species_link = soup.find('a', href=lambda x: x and '/species/' in x)

    if species_link:
        species_url = "https://www.iucnredlist.org" + species_link['href']

        # Get species page
        species_response = requests.get(species_url)
        species_soup = BeautifulSoup(species_response.text, 'html.parser')

        # Find PDF download link
        pdf_link = species_soup.find('a', text='Download PDF')

        if pdf_link:
            pdf_url = pdf_link['href']
            # Download PDF
            ...
```

**Estimated Success:** 300-400 PDFs (30-40% of 1,104)

---

#### Strategy B: Scrape Regional Reports

**IUCN Regional Red Lists:**
- Mediterranean Sea Elasmobranchs
- European Elasmobranchs
- Arabian Seas & Adjacent Waters
- Eastern Atlantic & Mediterranean
- Pacific Islands

**Location:** https://www.iucnredlist.org/resources/regional-red-lists

**Approach:**
1. Identify regional report papers in database by title keywords
2. Download regional assessment PDFs from IUCN resources page
3. Match to database papers by title/region

**Estimated Success:** 50-100 PDFs

---

#### Strategy C: Direct PDF URL Construction (if pattern exists)

Some IUCN PDFs may follow predictable URL patterns:
```
https://www.iucnredlist.org/pdf/{taxon_id}
https://nc.iucnredlist.org/redlist/resources/files/{taxon_id}/Assessment_Name.pdf
```

**Approach:**
1. Get taxon IDs from IUCN API
2. Test URL patterns with known species
3. Bulk download if pattern works

**Estimated Success:** Uncertain, depends on URL pattern existence

---

### Recommended Actions for Priority 4

#### Immediate (1-2 days)
1. **Manual test of web scraping approach:**
   - Pick 5 species from failed attempts
   - Manually search IUCN website
   - Document PDF availability and URL patterns
   - Test if PDFs are freely downloadable

2. **Build web scraper prototype:**
   - Implement Strategy A (individual species)
   - Test on 20 papers
   - Measure success rate

#### Short-term (3-5 days)
3. **Full web scraping implementation:**
   - Scrape all 1,104 papers
   - Implement rate limiting (2-3 seconds between requests)
   - Handle failed lookups gracefully

4. **Regional reports:**
   - Download all regional Red List PDFs from IUCN resources
   - Match to database papers

**Expected Yield:** 350-500 PDFs (30-45% of 1,104 papers)

---

## Overall Recommendations

### Critical Path Forward

**Week 1: Quick Wins**
1. ‚úÖ **Semantic Scholar** - Extract 19 DOIs (30 minutes)
2. üî¥ **Sci-Hub** - Test from different network (30 minutes)
3. üü° **IUCN** - Manual test web scraping approach (2 hours)

**Week 2: Implementation**
4. üî¥ **Sci-Hub** - Implement Tor solution if still blocked (2 days)
5. üü° **IUCN** - Build and run web scraper (3 days)

**Week 3: Bulk Downloads**
6. üî¥ **Sci-Hub** - Re-run 11,858 DOIs via Tor (~2-3 days runtime)
7. üü° **IUCN** - Complete scraping and organize PDFs

### Expected Outcomes

| Priority | Time Investment | Expected Gain | Notes |
|----------|----------------|---------------|-------|
| Sci-Hub (with Tor) | 4-5 days | +8,000 PDFs | Highest impact |
| IUCN web scraping | 3-5 days | +350-500 PDFs | Good ROI |
| Semantic Scholar DOIs | 30 minutes | +10-15 PDFs | Manual follow-up |
| **TOTAL** | **2 weeks** | **+8,350-8,515 PDFs** | **Would bring total to ~13,750-13,900 (46%)** |

---

## Alternative Strategy: Focus on Other Priorities

Given Sci-Hub is blocked and IUCN requires development, we could pivot to **other acquisition strategies** from the original list:

### Priority 3: NOAA Papers (Low Effort, Quick Win)
- **130 papers** waiting for government sites to come back online
- **Expected: +120 PDFs** (90% success rate)
- **Effort:** Just retry when sites are up
- **Monitor:** fishbull.noaa.gov, spo.nwr.noaa.gov

### Priority 5: Thesis Downloads (Medium Effort, Good Yield)
- **~500 thesis papers** in database
- **Script exists:** `scripts/download_theses_multisource.py`
- **Current status:** 325 logged, unclear success rate
- **Expected: +300-400 PDFs** (60-80% success rate)
- **Effort:** 2-3 days to optimize and expand

### Priority 6: ResearchGate/Academia.edu (High Effort, Medium Yield)
- **Browser automation needed** (Selenium/Playwright)
- **Expected: +500-1,000 PDFs** from papers not found elsewhere
- **Effort:** 5-7 days development
- **Requires:** Manual CAPTCHA solving or authentication

---

## Files Created/Referenced

**Documentation:**
- `docs/database/scihub_diagnosis.md` - Detailed Sci-Hub blocking analysis
- `docs/database/acquisition_strategy_review.md` - Overall strategy document
- `docs/database/priorities_1_2_4_investigation_summary.md` - This file

**Logs Analyzed:**
- `logs/scihub_download_log.csv` - 11,858 attempts, 13.6% success
- `logs/semantic_scholar_log.csv` - 13,890 searches, 0.17% success
- `logs/iucn_download_log.csv` - 1,104 attempts, 0% success

**Scripts:**
- `scripts/download_via_scihub.py` - Needs Tor integration
- `scripts/search_semantic_scholar.py` - Completed, low yield
- `scripts/download_iucn_assessments.py` - API approach failed, needs web scraping

---

## Decision Point

**Which path to take?**

### Option A: Fix Blocking Issues (Recommended for Max Impact)
- Implement Tor for Sci-Hub (+8,000 PDFs)
- Build IUCN web scraper (+350-500 PDFs)
- **Total gain: ~8,500 PDFs in 2-3 weeks**

### Option B: Pursue Easier Wins (Recommended for Quick Progress)
- Wait for NOAA sites (+120 PDFs, 0 effort)
- Optimize thesis downloads (+300-400 PDFs, 3 days)
- Manual institutional downloads (+500-1,000 PDFs, ongoing)
- **Total gain: ~1,000-1,500 PDFs in 1-2 weeks**

### Option C: Hybrid Approach (Balanced)
- Implement Tor for Sci-Hub in parallel (background task)
- Work on thesis optimization (active task)
- Monitor NOAA sites (passive)
- **Total gain: ~8,500-9,000 PDFs in 2-3 weeks**

**Recommendation:** **Option C (Hybrid)** - Maximizes total yield while maintaining momentum

---

**Status:** Investigation complete, awaiting decision on next actions

**Next Actions:** Choose path forward and begin implementation
